{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.1 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kURC-RTZdb2k"
      },
      "source": [
        "This homework is based off a notebook. \r\n",
        "1. You will alter this as need for your results / questions. \r\n",
        "2. When you've successfully run them on a colab, download as an .ipynb and submit on the LMS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZH14NyOLir"
      },
      "source": [
        "Make certain you are on a GPU Runtime first, by going to Runtime and selecting \"Change Runtime Type\", and then choosing Hardware Accelerator as GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y679JqaaBxuT"
      },
      "source": [
        "In this homework, you will build an emotion classifier based on a Huggingface emotions dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "You will need to install 🤗 Transformers, numpy and 🤗 Datasets. Run the following three cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-1.8.0-cp39-none-macosx_10_9_x86_64.whl (120.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 120.6 MB 10.0 kB/s \n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.9.0-cp39-cp39-macosx_10_9_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (1.20.1)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting pillow>=4.1.1\n",
            "  Downloading Pillow-8.1.2-cp39-cp39-macosx_10_10_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: typing-extensions, torch, pillow, torchvision\n",
            "Successfully installed pillow-8.1.2 torch-1.8.0 torchvision-0.9.0 typing-extensions-3.7.4.3\n",
            "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.5.0-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 782 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from optuna) (4.49.0)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.5.6-py2.py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-4.7.2-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from optuna) (20.9)\n",
            "Collecting sqlalchemy>=1.1.0\n",
            "  Downloading SQLAlchemy-1.3.23-cp39-cp39-macosx_10_14_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from optuna) (1.6.1)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from optuna) (1.20.1)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.7.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from optuna) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil in /Users/sarimkhans/Library/Python/3.9/lib/python/site-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-1.5.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=3.12\n",
            "  Downloading PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
            "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.5.1-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/sarimkhans/Library/Python/3.9/lib/python/site-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Collecting MarkupSafe>=0.9.2\n",
            "  Downloading MarkupSafe-1.1.1-cp39-cp39-macosx_10_9_x86_64.whl (16 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Collecting attrs>=16.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /Users/sarimkhans/Library/Python/3.9/lib/python/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Using legacy 'setup.py install' for pyperclip, since package 'wheel' is not installed.\n",
            "Installing collected packages: cmaes, python-editor, sqlalchemy, MarkupSafe, Mako, alembic, colorlog, pyperclip, attrs, colorama, cmd2, PyYAML, PrettyTable, pbr, stevedore, cliff, optuna\n",
            "    Running setup.py install for pyperclip ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed Mako-1.1.4 MarkupSafe-1.1.1 PrettyTable-2.1.0 PyYAML-5.4.1 alembic-1.5.6 attrs-20.3.0 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-4.7.2 optuna-2.5.0 pbr-5.5.1 pyperclip-1.8.2 python-editor-1.0.4 sqlalchemy-1.3.23 stevedore-3.3.0\n",
            "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sklearn) (0.24.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.20.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.6.1)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "pip install datasets==1.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==1.3.0\n",
            "  Downloading datasets-1.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 798 kB/s \n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting pyarrow>=0.17.1\n",
            "  Downloading pyarrow-3.0.0-cp39-cp39-macosx_10_13_x86_64.whl (14.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2 MB 776 kB/s \n",
            "\u001b[?25hCollecting numpy>=1.17\n",
            "  Downloading numpy-1.20.1-cp39-cp39-macosx_10_9_x86_64.whl (16.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.0.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 938 kB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 1.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.2\n",
            "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets==1.3.0) (2.25.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.11.1-py39-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 845 kB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.2.3-cp39-cp39-macosx_10_9_x86_64.whl (10.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.7 MB 715 kB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.3.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.3.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.3.0) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.3.0) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/sarimkhans/Library/Python/3.9/lib/python/site-packages (from pandas->datasets==1.3.0) (2.8.1)\n",
            "Collecting pytz>=2017.3\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 754 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/sarimkhans/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.3.0) (1.15.0)\n",
            "Using legacy 'setup.py install' for xxhash, since package 'wheel' is not installed.\n",
            "Installing collected packages: dill, numpy, pyarrow, xxhash, fsspec, tqdm, filelock, huggingface-hub, multiprocess, pytz, pandas, datasets\n",
            "    Running setup.py install for xxhash ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed datasets-1.3.0 dill-0.3.3 filelock-3.0.12 fsspec-0.8.7 huggingface-hub-0.0.2 multiprocess-0.70.11.1 numpy-1.20.1 pandas-1.2.3 pyarrow-3.0.0 pytz-2021.1 tqdm-4.49.0 xxhash-2.0.0\n",
            "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9lgfCA1OC0M"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (4.3.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (2020.11.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (4.49.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (1.20.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: six in /Users/sarimkhans/Library/Python/3.9/lib/python/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (2020.12.5)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63YCXee080As"
      },
      "source": [
        "pip install numpy==1.20.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.20.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.20.1)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "source": [
        "from datasets import load_dataset\n",
        "emotions_dataset = load_dataset('go_emotions', 'simplified')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset go_emotions (/Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZy5tRB_IrI7"
      },
      "source": [
        "# For simplicity, train model with single label for item.\n",
        "restricted_dataset = emotions_dataset.filter(lambda x: len(x[\"labels\"]) == 1 and 27 not in x['labels'])\n",
        "print(len(restricted_dataset['train']))\n",
        "show_random_elements(restricted_dataset['train'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-30532e9cc7b8a9d6.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-d93795d3697d275e.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-9832485185737d89.arrow\n",
            "23485\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>labels</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ef5fnn9</td>\n      <td>[17]</td>\n      <td>Glad they made it..</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eeyfzvw</td>\n      <td>[15]</td>\n      <td>Thank you for commenting exactly what I was going to comment and saving me the trouble!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>edw9jvd</td>\n      <td>[0]</td>\n      <td>Real nice, swearing up a storm right in front of your kids.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>eebeui2</td>\n      <td>[10]</td>\n      <td>No, it was believed that the baby absorbed maternal traits because the mother was the \"soil\" the \"seed\" grew in.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eehewzl</td>\n      <td>[9]</td>\n      <td>“Regain the fan base”. How immature do you have to be to believe that everyone agrees with you.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ed9y743</td>\n      <td>[20]</td>\n      <td>Yeah, hopefully the lesson learned from this is that our level of airport security is stupid and unnecessary and not that [NAME] is risking another 9/11</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ed62fhl</td>\n      <td>[4]</td>\n      <td>Yup, I'm sick with a mild cold</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ed2zfwd</td>\n      <td>[6]</td>\n      <td>Why is it dumb? It's not like once people get married they believe in a society that looks down on premarital sex. Maybe you missed my point.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>edc2z55</td>\n      <td>[0]</td>\n      <td>Well good...opera is often beautiful..though it retains the stink of the upper classes.. I cant help but deride art that caters to high society to stay alive.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>eeowi6x</td>\n      <td>[5]</td>\n      <td>It may get you pregnant so just make sure you eat all her birth control before hand.</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "\r\n",
        "The emotions are provided as numeric labels. These are the actual orderings, beginning at 0 for admiration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o4rUteaIrI_"
      },
      "source": [
        "labels = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \n",
        "          \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\",\n",
        "          \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\",\n",
        "          \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\",\n",
        "          \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
        "index_to_labels = {index: label for index, label in enumerate(labels)}\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "model_init= AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=27)\n",
        "#config = AutoConfig.from_pretrained(model_name, num_labels=27)\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the 🤗 Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HakAebh53HGR"
      },
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True)\n",
        "\n",
        "def convert_labels_to_int(example):\n",
        "    example['labels'] = example['labels'][0]\n",
        "    return example\n",
        "\n",
        "encoded_dataset = restricted_dataset.map(preprocess_function, batched=True)\n",
        "encoded_dataset = encoded_dataset.map(convert_labels_to_int)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-cc7fe283fbd8c8d9.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-4caeb60e4efe21b3.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-83caf1489f390fdb.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-adf2e41c49e2c59e.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-b815433057a8493b.arrow\n",
            "Loading cached processed dataset at /Users/sarimkhans/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e/cache-150152e9e0d83b41.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define two more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "metric_name = \"accuracy\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"test-emotions\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the `Trainer` to load the best model it saved (according to `metric_name`) at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "source": [
        "metric = datasets.load_metric('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "source": [
        "\n",
        "validation_key = \"validation\"\n",
        "trainer = Trainer(\n",
        "    model_init,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[validation_key],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWGmvxbIrJg"
      },
      "source": [
        "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it once last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive the samples like the dictionaries seen above and will need to return a dictionary of tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "import numpy as np\n",
        "trainer.train()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9, 15.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████▏  | 656/920 [149:28:14<1:10:14, 15.96s/it]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████▏  | 657/920 [149:28:30<1:10:04, 15.99s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 658/920 [149:28:46<1:08:53, 15.78s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 659/920 [149:29:03<1:10:39, 16.25s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 660/920 [149:29:18<1:08:10, 15.73s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 661/920 [149:29:32<1:06:20, 15.37s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 662/920 [149:29:47<1:05:32, 15.24s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 663/920 [149:30:03<1:06:38, 15.56s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 664/920 [149:30:18<1:05:54, 15.45s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 665/920 [149:30:33<1:04:16, 15.12s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 666/920 [149:30:48<1:03:47, 15.07s/it]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▎  | 667/920 [149:31:03<1:03:42, 15.11s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 668/920 [149:31:18<1:02:56, 14.99s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 669/920 [149:31:32<1:01:59, 14.82s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 670/920 [149:31:48<1:03:40, 15.28s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 671/920 [149:32:03<1:03:03, 15.20s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 672/920 [149:32:20<1:03:54, 15.46s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 673/920 [149:32:34<1:01:49, 15.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 674/920 [149:32:50<1:02:49, 15.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 675/920 [149:33:05<1:02:35, 15.33s/it]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 676/920 [149:33:20<1:01:59, 15.24s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▎  | 677/920 [149:33:37<1:03:18, 15.63s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▎  | 678/920 [149:33:52<1:02:43, 15.55s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 679/920 [149:34:07<1:01:49, 15.39s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 680/920 [149:34:20<59:04, 14.77s/it]  \u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 681/920 [149:34:35<59:13, 14.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 682/920 [149:34:50<59:06, 14.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 683/920 [149:35:05<58:52, 14.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 684/920 [149:35:23<1:02:27, 15.88s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 685/920 [149:35:39<1:01:24, 15.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 686/920 [149:35:53<59:23, 15.23s/it]  \u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 687/920 [149:36:08<58:46, 15.13s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 688/920 [149:36:24<59:36, 15.42s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 689/920 [149:36:38<58:21, 15.16s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 690/920 [149:36:52<56:05, 14.63s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 691/920 [149:37:06<55:23, 14.51s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 692/920 [149:37:19<53:56, 14.19s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 693/920 [149:37:33<52:47, 13.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 694/920 [149:37:47<52:50, 14.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 695/920 [149:38:01<52:26, 13.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 696/920 [149:38:17<54:09, 14.51s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 697/920 [149:38:33<55:38, 14.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 698/920 [149:38:47<54:24, 14.71s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 699/920 [149:39:01<53:34, 14.55s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 700/920 [149:39:18<56:22, 15.37s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 701/920 [149:39:33<55:34, 15.22s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▋  | 702/920 [149:39:47<54:24, 14.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▋  | 703/920 [149:40:03<54:34, 15.09s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 704/920 [149:40:20<56:34, 15.72s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 705/920 [149:40:36<56:16, 15.71s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 706/920 [149:40:51<55:08, 15.46s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 707/920 [149:41:12<1:01:11, 17.24s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 708/920 [149:41:28<1:00:03, 17.00s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 709/920 [149:41:43<57:42, 16.41s/it]  \u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 710/920 [149:41:58<55:32, 15.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 711/920 [149:42:15<56:01, 16.08s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 712/920 [149:42:30<55:18, 15.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 713/920 [149:42:47<55:21, 16.05s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 714/920 [149:43:02<54:01, 15.73s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 715/920 [149:43:16<51:59, 15.22s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 716/920 [149:43:27<48:22, 14.23s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 717/920 [149:43:40<46:02, 13.61s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 718/920 [149:43:53<45:21, 13.47s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 719/920 [149:44:05<44:14, 13.20s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 720/920 [149:44:18<42:58, 12.89s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 721/920 [149:44:31<43:10, 13.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 722/920 [149:44:43<42:04, 12.75s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▊  | 723/920 [149:44:55<40:53, 12.46s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▊  | 724/920 [149:45:07<40:57, 12.54s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 725/920 [149:45:21<41:26, 12.75s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 726/920 [149:45:34<41:48, 12.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 727/920 [149:45:48<42:16, 13.14s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 728/920 [149:46:00<41:07, 12.85s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 729/920 [149:46:17<45:13, 14.21s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 730/920 [149:46:32<45:02, 14.22s/it]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 731/920 [149:46:44<42:53, 13.61s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 732/920 [149:46:57<41:59, 13.40s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 733/920 [149:47:09<40:40, 13.05s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 734/920 [149:47:23<41:39, 13.44s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 735/920 [149:47:39<43:16, 14.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 736/920 [149:47:45<35:35, 11.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 2/24 [00:03<00:43,  1.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▎        | 3/24 [00:08<00:54,  2.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 4/24 [00:12<01:00,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 5/24 [00:16<01:04,  3.38s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▌       | 6/24 [00:20<01:05,  3.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 7/24 [00:24<01:04,  3.82s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 8/24 [00:29<01:04,  4.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 9/24 [00:33<00:59,  3.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 10/24 [00:36<00:54,  3.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 11/24 [00:40<00:51,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 12/24 [00:44<00:47,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 13/24 [00:48<00:43,  3.99s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 14/24 [00:52<00:39,  3.95s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▎   | 15/24 [00:56<00:35,  3.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 16/24 [01:00<00:31,  3.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 17/24 [01:05<00:28,  4.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▌  | 18/24 [01:09<00:25,  4.29s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 19/24 [01:13<00:20,  4.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 20/24 [01:18<00:16,  4.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 21/24 [01:22<00:12,  4.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 22/24 [01:26<00:08,  4.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 23/24 [01:29<00:04,  4.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 24/24 [01:30<00:00,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/11744 [150:54:26<?, ?it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 736/920 [149:49:19<35:35, 11.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/7340 [150:48:22<?, ?it/s]\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A{'eval_loss': 1.289544701576233, 'eval_accuracy': 0.6468200270635994, 'eval_runtime': 94.5462, 'eval_samples_per_second': 31.265, 'epoch': 4.0}\n",
            "\n",
            "\n",
            " 80%|████████  | 737/920 [149:49:35<2:05:55, 41.29s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 738/920 [149:49:52<1:42:44, 33.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 739/920 [149:50:09<1:27:01, 28.85s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 740/920 [149:50:25<1:15:20, 25.11s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 741/920 [149:50:42<1:07:34, 22.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 742/920 [149:50:59<1:02:24, 21.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 743/920 [149:51:13<55:12, 18.71s/it]  \u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 744/920 [149:51:28<51:43, 17.63s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 745/920 [149:51:43<49:00, 16.80s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 746/920 [149:52:00<49:35, 17.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 747/920 [149:52:15<46:59, 16.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████▏ | 748/920 [149:52:32<47:15, 16.49s/it]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████▏ | 749/920 [149:52:50<48:16, 16.94s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 750/920 [149:53:05<46:48, 16.52s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 751/920 [149:53:21<45:31, 16.16s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 752/920 [149:53:37<45:16, 16.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 753/920 [149:53:52<43:56, 15.79s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 754/920 [149:54:10<45:24, 16.41s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 755/920 [149:54:26<45:23, 16.50s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 756/920 [149:54:42<44:36, 16.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 757/920 [149:54:59<45:03, 16.58s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 758/920 [149:55:15<43:38, 16.16s/it]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▎ | 759/920 [149:55:31<43:47, 16.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 760/920 [149:55:48<43:29, 16.31s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 761/920 [149:56:04<43:29, 16.41s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 762/920 [149:56:19<41:55, 15.92s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 763/920 [149:56:34<41:00, 15.67s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 764/920 [149:56:49<40:15, 15.48s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 765/920 [149:57:05<40:40, 15.75s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 766/920 [149:57:21<40:30, 15.78s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 767/920 [149:57:38<40:36, 15.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 768/920 [149:57:53<39:59, 15.79s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▎ | 769/920 [149:58:09<39:55, 15.86s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▎ | 770/920 [149:58:26<40:36, 16.24s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 771/920 [149:58:41<39:28, 15.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 772/920 [149:59:00<41:03, 16.64s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 773/920 [149:59:15<39:33, 16.14s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 774/920 [149:59:30<38:24, 15.78s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 775/920 [149:59:45<37:54, 15.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 776/920 [150:00:02<38:30, 16.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 777/920 [150:00:19<38:40, 16.22s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 778/920 [150:00:35<38:49, 16.41s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 779/920 [150:00:54<40:13, 17.12s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 780/920 [150:01:12<40:35, 17.40s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 781/920 [150:01:29<40:01, 17.28s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 782/920 [150:01:48<40:52, 17.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 783/920 [150:02:04<39:30, 17.31s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 784/920 [150:02:22<39:12, 17.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 785/920 [150:02:38<38:03, 16.92s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 786/920 [150:02:53<36:33, 16.37s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 787/920 [150:03:09<35:52, 16.19s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 788/920 [150:03:23<34:40, 15.76s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 789/920 [150:03:41<35:33, 16.28s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 790/920 [150:03:57<35:23, 16.34s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 791/920 [150:04:19<38:35, 17.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 792/920 [150:04:36<37:29, 17.58s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 793/920 [150:04:52<36:04, 17.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▋ | 794/920 [150:05:10<36:53, 17.57s/it]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▋ | 795/920 [150:05:28<36:36, 17.57s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 796/920 [150:06:09<51:11, 24.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 797/920 [150:06:25<45:04, 21.99s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 798/920 [150:06:41<41:22, 20.35s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 799/920 [150:06:58<38:53, 19.28s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 800/920 [150:07:14<36:16, 18.14s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 801/920 [150:07:35<37:38, 18.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 802/920 [150:07:51<35:51, 18.23s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 803/920 [150:08:06<33:21, 17.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 804/920 [150:08:22<32:30, 16.82s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 805/920 [150:08:38<32:05, 16.74s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 806/920 [150:08:54<30:57, 16.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 807/920 [150:09:08<29:27, 15.64s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 808/920 [150:09:24<29:20, 15.72s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 809/920 [150:09:39<28:53, 15.62s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 810/920 [150:09:55<28:43, 15.67s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 811/920 [150:10:10<28:19, 15.59s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 812/920 [150:10:27<28:26, 15.80s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 813/920 [150:10:42<27:55, 15.66s/it]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 814/920 [150:10:56<27:01, 15.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▊ | 815/920 [150:11:12<26:53, 15.37s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▊ | 816/920 [150:11:30<27:52, 16.08s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 817/920 [150:11:45<27:16, 15.89s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 818/920 [150:12:01<26:58, 15.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 819/920 [150:12:16<26:07, 15.52s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 820/920 [150:12:31<25:50, 15.51s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 821/920 [150:12:49<26:41, 16.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 822/920 [150:13:06<26:42, 16.35s/it]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 823/920 [150:13:20<25:39, 15.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|████████▉ | 824/920 [150:13:35<24:46, 15.48s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|████████▉ | 825/920 [150:13:50<24:33, 15.51s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|████████▉ | 826/920 [150:14:07<24:37, 15.71s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|████████▉ | 827/920 [150:14:22<24:03, 15.52s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 828/920 [150:14:38<24:04, 15.70s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 829/920 [150:14:53<23:43, 15.64s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 830/920 [150:15:10<24:02, 16.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 831/920 [150:15:27<24:12, 16.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 832/920 [150:15:45<24:28, 16.69s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 833/920 [150:16:04<25:06, 17.31s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 834/920 [150:16:20<24:20, 16.99s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 835/920 [150:16:35<23:07, 16.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 836/920 [150:16:50<22:24, 16.00s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 837/920 [150:17:06<22:06, 15.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 838/920 [150:17:20<21:07, 15.46s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 839/920 [150:17:35<20:39, 15.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████▏| 840/920 [150:17:50<20:19, 15.25s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████▏| 841/920 [150:18:06<20:28, 15.55s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 842/920 [150:18:22<20:13, 15.55s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 843/920 [150:18:40<21:09, 16.49s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 844/920 [150:18:57<20:49, 16.44s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 845/920 [150:19:14<20:39, 16.53s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 846/920 [150:19:31<20:34, 16.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 847/920 [150:19:47<20:15, 16.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 848/920 [150:20:04<20:12, 16.84s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 849/920 [150:20:28<22:22, 18.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 850/920 [150:20:47<21:54, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▎| 851/920 [150:21:08<22:37, 19.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 852/920 [150:21:24<20:55, 18.46s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 853/920 [150:21:39<19:31, 17.49s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 854/920 [150:21:53<18:02, 16.40s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 855/920 [150:22:09<17:39, 16.29s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 856/920 [150:22:25<17:09, 16.09s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 857/920 [150:22:41<16:57, 16.15s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 858/920 [150:22:54<15:46, 15.26s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 859/920 [150:23:09<15:25, 15.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 860/920 [150:23:24<14:58, 14.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▎| 861/920 [150:23:43<15:58, 16.25s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▎| 862/920 [150:24:03<16:48, 17.39s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 863/920 [150:24:21<16:41, 17.57s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 864/920 [150:24:37<15:57, 17.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 865/920 [150:24:54<15:44, 17.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 866/920 [150:25:11<15:26, 17.16s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 867/920 [150:25:27<14:40, 16.62s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 868/920 [150:25:42<14:05, 16.25s/it]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 869/920 [150:25:57<13:27, 15.83s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 870/920 [150:26:13<13:09, 15.79s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 871/920 [150:26:26<12:17, 15.06s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 872/920 [150:26:42<12:15, 15.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 873/920 [150:27:00<12:34, 16.05s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 874/920 [150:27:14<11:54, 15.53s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 875/920 [150:27:30<11:40, 15.57s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 876/920 [150:27:46<11:31, 15.71s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 877/920 [150:28:02<11:20, 15.83s/it]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 878/920 [150:28:18<11:03, 15.79s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 879/920 [150:28:33<10:41, 15.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 880/920 [150:28:48<10:20, 15.51s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 881/920 [150:29:04<10:13, 15.72s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 882/920 [150:29:18<09:33, 15.09s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 883/920 [150:29:32<09:10, 14.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 884/920 [150:29:48<09:00, 15.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 885/920 [150:30:01<08:30, 14.60s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▋| 886/920 [150:30:16<08:17, 14.64s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▋| 887/920 [150:30:32<08:15, 15.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 888/920 [150:30:48<08:09, 15.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 889/920 [150:31:02<07:45, 15.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 890/920 [150:31:15<07:09, 14.32s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 891/920 [150:31:28<06:48, 14.08s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 892/920 [150:31:41<06:21, 13.62s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 893/920 [150:31:54<06:00, 13.36s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 894/920 [150:32:08<05:54, 13.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 895/920 [150:32:22<05:40, 13.61s/it]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 896/920 [150:32:37<05:38, 14.09s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 897/920 [150:32:53<05:38, 14.73s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 898/920 [150:33:07<05:21, 14.64s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 899/920 [150:33:24<05:18, 15.16s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 900/920 [150:33:39<05:01, 15.09s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 901/920 [150:33:53<04:44, 14.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 902/920 [150:34:06<04:13, 14.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 903/920 [150:34:19<03:54, 13.79s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 904/920 [150:34:33<03:41, 13.87s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 905/920 [150:34:46<03:25, 13.72s/it]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 906/920 [150:34:59<03:07, 13.42s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▊| 907/920 [150:35:11<02:49, 13.00s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▊| 908/920 [150:35:23<02:33, 12.80s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 909/920 [150:35:37<02:22, 13.00s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 910/920 [150:35:51<02:15, 13.50s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 911/920 [150:36:05<02:02, 13.61s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 912/920 [150:36:18<01:46, 13.31s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 913/920 [150:36:33<01:38, 14.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 914/920 [150:36:45<01:20, 13.40s/it]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 915/920 [150:36:59<01:07, 13.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|█████████▉| 916/920 [150:37:12<00:53, 13.41s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|█████████▉| 917/920 [150:37:26<00:40, 13.39s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|█████████▉| 918/920 [150:37:40<00:27, 13.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|█████████▉| 919/920 [150:37:55<00:13, 13.97s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 920/920 [150:38:01<00:00, 11.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 2/24 [00:04<00:44,  2.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▎        | 3/24 [00:08<00:55,  2.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 4/24 [00:12<01:01,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 5/24 [00:16<01:04,  3.41s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|██▌       | 6/24 [00:20<01:05,  3.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|██▉       | 7/24 [00:24<01:05,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 8/24 [00:29<01:05,  4.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 9/24 [00:33<01:00,  4.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 10/24 [00:37<00:55,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 11/24 [00:41<00:52,  4.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 12/24 [00:45<00:48,  4.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 13/24 [00:49<00:44,  4.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 14/24 [00:53<00:40,  4.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▎   | 15/24 [00:57<00:36,  4.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|██████▋   | 16/24 [01:01<00:32,  4.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|███████   | 17/24 [01:06<00:29,  4.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▌  | 18/24 [01:11<00:26,  4.37s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|███████▉  | 19/24 [01:14<00:21,  4.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|████████▎ | 20/24 [01:19<00:17,  4.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 21/24 [01:23<00:12,  4.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 22/24 [01:27<00:08,  4.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 23/24 [01:31<00:04,  4.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 24/24 [01:31<00:00,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/11744 [151:44:43<?, ?it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 920/920 [150:39:37<00:00, 11.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/7340 [151:38:40<?, ?it/s]\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A{'eval_loss': 1.2820581197738647, 'eval_accuracy': 0.6468200270635994, 'eval_runtime': 96.118, 'eval_samples_per_second': 30.754, 'epoch': 5.0}\n",
            "\n",
            "\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/11744 [151:44:47<?, ?it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 920/920 [150:39:40<00:00, 589.54s/it]{'train_runtime': 542380.5423, 'train_samples_per_second': 0.002, 'epoch': 5.0}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=920, training_loss=1.5183068980341372, metrics={'train_runtime': 542380.5423, 'train_samples_per_second': 0.002, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKASz-2vIrJi"
      },
      "source": [
        "We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one). We also run a sample prediction to demonstrate the API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOUcBkX8IrJi"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f245b31d31e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6-FmS4d1mDE"
      },
      "source": [
        "prepared_input = tokenizer.prepare_seq2seq_batch([\"I am very sad\"], return_tensors='pt')\r\n",
        "model = model.to('cpu')\r\n",
        "model.eval()\r\n",
        "model_output = model(**prepared_input)\r\n",
        "prediction = np.argmax(model_output.logits[0].detach().numpy())\r\n",
        "index_to_labels[prediction]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-1Jo6jShRwF"
      },
      "source": [
        "# Your work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YhB0mvh01au"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQGD4MZhhsB1"
      },
      "source": [
        "Qs 1. Can you report how accuracy changes with varying the hypyerparameters defined above? How does accuracy change with increasing number of epochs? Learning rate? Mention your final choice of parameters and graph how they impact accuracy. \r\n",
        "\r\n",
        "Qs 2. Try any 2 other models for sequence classification supported by Huggingface and repeat Qs 1 for these. You can find a full list of these [here](https://huggingface.co/transformers/model_doc/auto.html#automodelforsequenceclassification). (For those of you working with text for your project, you may also find several other useful models for Seq2Seq modeling, question answering, etc.)\r\n",
        "\r\n",
        "Qs 3. Wrangle / change the data so that you can instead build a \"positive\" emotion vs \"negative\" emotions classifier, by combining labels. For any emotions that you feel are too neutral, you may drop them from evaluation. Can you now report the accuracy of a classifer on this data? You may use one of the model architectures from (1) or (2)\r\n",
        "\r\n",
        "Qs 4. Export models from qs 1 and 3, and build a FastAPI around it, as discussed in class, which returns the most likely emotion found. (You may need to look up Huggingface documentation on how to save and load models). In particular, you should support two API endpoints - \"detect_emotion_binary\" and \"detect_emotion_full\" which supports each model.\r\n",
        "\r\n",
        "Qs 5. Finally, can you dockerize this? You do NOT need to deploy this to the cloud.\r\n",
        "\r\n",
        "Submission instructions: Zip your compiled .ipynb notebook, your fast_api code and your dockerfile, and submit to LMS as one file. You should confirm that your .ipynb can be imported to colab and run in sequence. All your results for Qs 1-3 should be reported in the notebook itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5VeU4ob4YF6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}